{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix model checkpoint at each epoch - FIXED\n",
    "# fix tensorboard - FIXED\n",
    "# make in advance a model and py script for each training set\n",
    "# testing and plotting to understand results\n",
    "\n",
    "# test how much r2 are we achieving with only sequences - so only LSTM to output, \n",
    "# although still convolution to accelerate learning\n",
    "\n",
    "# approx 180 epochs training - > training over 100 does increase performance\n",
    "# have to implement R2 as performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "from time import time\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Conv1D,MaxPooling1D,LSTM,BatchNormalization,Dropout,Input,Dense,Bidirectional,Activation,Add,Concatenate,Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import squeeze\n",
    "from keras.utils import io_utils, HDF5Matrix\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if running notebook or standalone python script\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "import configparser\n",
    "import sys\n",
    "args = (sys.argv)\n",
    "\n",
    "model_name = \"THIS_model\"\n",
    "DROPOUT = 0   # dropout\n",
    "ALPHA = 0.01 # learnrate\n",
    "BETA = 0.01\n",
    "EPOCHS = 2 # epochs\n",
    "MBATCH = 100 # batch size\n",
    "\n",
    "#loading from config \n",
    "if not is_interactive():\n",
    "    config_file = args[1]\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    ALPHA = config.getfloat('main', 'alpha')\n",
    "    DROPOUT = config.getfloat('main', 'dropout')\n",
    "    EPOCHS = config.getint('main', 'epochs')\n",
    "    MBATCH = config.getint('main', 'mbatch')\n",
    "    BETA = config.getfloat('main', 'beta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4006, 650, 4)\n",
      "(4006, 69)\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "data_path = \"../data/THIS_data.npz\"\n",
    "\n",
    "if not is_interactive():\n",
    "    data_path = \"./data/THIS_data.npz\"\n",
    "npzfile = np.load(data_path)\n",
    "Xh_train = npzfile['arr_0']\n",
    "Xh_test = npzfile['arr_1']\n",
    "Xv_train = npzfile['arr_2']\n",
    "Xv_test = npzfile['arr_3']\n",
    "Y_train = npzfile['arr_4']\n",
    "Y_test = npzfile['arr_5']\n",
    "\n",
    "print(Xh_train.shape)\n",
    "print(Xv_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape1 = Xh_train.shape\n",
    "#shape2 = Xv_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: Numpy array of training data (if the model has a single input), or list of Numpy arrays \n",
    "# (if the model has multiple inputs). If input layers in the model are named, you can also \n",
    "# pass a dictionary mapping input names to Numpy arrays. \n",
    "X_train = list()\n",
    "X_train.append(Xh_train)\n",
    "#X_train.append(Xv_train)\n",
    "\n",
    "X_test = list()\n",
    "X_test.append(Xh_test)\n",
    "#X_test.append(Xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POC_model_2(input_shape_hot):\n",
    "    # model has 2 sets of separate input layers\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input1 = Input(shape = input_shape_hot)\n",
    "    #X_input2 = Input(shape = input_shape_val)\n",
    "    DR = DROPOUT\n",
    "    \n",
    "    ## Step 1 - hot layers\n",
    "    # L 1: CONV \n",
    "    X1 = Conv1D(filters=16, kernel_size=10, strides=3)(X_input1) # size-kernel/stride + 1 = 640/2 + 1 = 321 out\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    X1 = Dropout(DR)(X1)   # dropout\n",
    "    \n",
    "    # L 2: Maxpool\n",
    "    X1 = MaxPooling1D(pool_size=16, strides=2)(X1) # 321/2 +1 = 162 (x 16) out                 \n",
    "    X1 = BatchNormalization()(X1)\n",
    "    #X = Activation('relu')(X)  # ReLu activation, based on other models from course\n",
    "    X1 = Dropout(DR)(X1)\n",
    "    \n",
    "    # L 3: Bidirectional LSTM\n",
    "    X1 = Bidirectional(LSTM(81, return_sequences=True))(X1) # 81 out\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    #X = Activation('relu')(X)\n",
    "    X1 = Dropout(DR)(X1) \n",
    "    \n",
    "    # L 3: Bidirectional LSTM\n",
    "    X1 = Bidirectional(LSTM(40, return_sequences=False))(X1) # 40 out\n",
    "    X1 = BatchNormalization()(X1)\n",
    "    #X = Activation('relu')(X)\n",
    "    X1 = Dropout(DR)(X1)\n",
    "    \n",
    "    # Step 2 - neural network merge data\n",
    "    # L1 -2 Dense layer for \n",
    "    #X = Concatenate(axis=1)([X1,X_input2])\n",
    "    X = Dense(20)(X1)  # 20 out\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(DR)(X) \n",
    "\n",
    "    # Step 3 - output\n",
    "    X = Dense(1)(X)\n",
    "    #X = Activation('softmax')(X) # activation depending on classification problem solved\n",
    "\n",
    "    model = Model(inputs = [X_input1], outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 650, 4)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 214, 16)           656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 214, 16)           64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 214, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 16)           64        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 162)          63504     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100, 162)          648       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 162)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 80)                64960     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 131,937\n",
      "Trainable params: 131,349\n",
      "Non-trainable params: 588\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = POC_model_2((650,4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true-K.mean(y_true)))\n",
    "    return (1-SS_res/(SS_tot+K.epsilon()))\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true, name='pearson_r')\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'pearson_r'  in i.name.split('/')]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        pearson_r = tf.identity(pearson_r)\n",
    "        return pearson_r*abs(pearson_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=ALPHA, beta_1=0.9, beta_2=0.999, decay=BETA)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=[coeff_determination, correlation_coefficient, 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callbacks\n",
    "# checkpoint\n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "# https://keras.io/callbacks/ - for now save every epoch\n",
    "suffix = \"{epoch:03d}-{val_loss:.3f}.hdf5\"\n",
    "filepath = \"{}.{}\".format(model_name, suffix)\n",
    "if not is_interactive():\n",
    "    model_name = args[2]\n",
    "\n",
    "    filepath = \"{}.{}\".format(model_name, suffix)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, \n",
    "                             save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# tensorboard\n",
    "# http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./tensorboard_logs/run1')\n",
    "\n",
    "csv_logger_filename = \"csvresults_log.csv\"\n",
    "if not is_interactive():\n",
    "    csv_logger_filename = args[3]\n",
    "csv = CSVLogger(csv_logger_filename, separator = \",\", append = True)\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard, csv]\n",
    "\n",
    "# in terminal run: tensorboard --logdir=logs/\n",
    "# val_loss error is in callbacks, probably modelcheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3605 samples, validate on 401 samples\n",
      "Epoch 1/2\n",
      "3605/3605 [==============================] - 32s 9ms/step - loss: 23.6841 - coeff_determination: -29.0525 - correlation_coefficient: 4.1726e-04 - mean_squared_error: 23.6841 - val_loss: 6.8327 - val_coeff_determination: -253877.2519 - val_correlation_coefficient: 4.4308e-06 - val_mean_squared_error: 6.8327\n",
      "Epoch 2/2\n",
      "3605/3605 [==============================] - 32s 9ms/step - loss: 2.5437 - coeff_determination: -2.2170 - correlation_coefficient: 9.7090e-05 - mean_squared_error: 2.5437 - val_loss: 4.4692 - val_coeff_determination: -75796.6565 - val_correlation_coefficient: -8.5414e-07 - val_mean_squared_error: 4.4692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x326314c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if model exist then load best\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def find_best_model(all_models):\n",
    "        epochs = []\n",
    "        losses = []\n",
    "        for i, file in enumerate(all_models):\n",
    "            groups = re.findall(model_name+'.(.*)-(.*).hdf5', file)\n",
    "            if groups:\n",
    "                epochs.append(int(groups[0][0]))\n",
    "                losses.append(float(groups[0][1]))\n",
    "        return (all_models[np.argmin(losses)] )\n",
    "\n",
    "all_models = glob.glob(model_name+'*.hdf5')\n",
    "\n",
    "if all_models:\n",
    "    best_model = find_best_model(all_models)\n",
    "    print(\"Loading weights from {}\".format(best_model))\n",
    "    model.load_weights(best_model)\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=MBATCH, epochs=EPOCHS, validation_split=0.1, shuffle=False, callbacks=callbacks_list)\n",
    "# keras model checkpoint KeyError: 'val_loss'\n",
    "# fix: https://github.com/keras-team/keras/issues/6104\n",
    "# must add validation_split=xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 [==============================] - 2s 4ms/step\n",
      "Dev set mse =  4.29579925109\n",
      "Dev set pearson =  -2.31320497851e-05\n",
      "Dev set r2 =  -5.87566538777\n"
     ]
    }
   ],
   "source": [
    "loss,r2, pearson, mse,  = model.evaluate(X_test, Y_test)\n",
    "print(\"Dev set mse = \", mse)\n",
    "print(\"Dev set pearson = \", pearson)\n",
    "print(\"Dev set r2 = \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "#print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "# https://stackoverflow.com/questions/893657/how-do-i-calculate-r-squared-using-python-and-numpy\n",
    "from scipy import stats\n",
    "slope, intercept, rvalue, pvalue, stderr = stats.linregress(Y_test[:,0], Y_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-squared: -0.0024365668515\n"
     ]
    }
   ],
   "source": [
    "print(\"r-squared:\", rvalue*abs(rvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
