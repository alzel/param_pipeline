{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6 modification\n",
    "\n",
    "# adds early stopping\n",
    "# FIXES learning rate sceheduler \n",
    "# -> drop by some amount every some epochs gradually\n",
    "# adds LRS = True/False condition to specify use of LR scheduler\n",
    "# changes model saving to best model + last model\n",
    "# loads model from last if exists\n",
    "\n",
    "# adds testing with simple random generated regression problem\n",
    "# https://www.kaggle.com/xgdbigdata/keras-regression-tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "## LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "\n",
    "if is_interactive():\n",
    "    import matplotlib\n",
    "    import pylab as plt\n",
    "else:\n",
    "    import matplotlib\n",
    "    matplotlib.use('agg')\n",
    "    import pylab as plt\n",
    "    \n",
    "import keras.backend as K\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=2, \n",
    "                                                   inter_op_parallelism_threads=2)))\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Conv1D,MaxPooling1D,LSTM,BatchNormalization,Dropout,Input,Dense,Bidirectional,Activation,Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import squeeze\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping, LearningRateScheduler\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "import random \n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to display train and validation metrics on same tensorboard plot\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./tensorboard_logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "        \n",
    "        # print learning rate for testing purposes\n",
    "        lr = float(K.get_value(self.model.optimizer.lr))\n",
    "        print(\"Learning rate:\", lr)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics and tests\n",
    "\n",
    "# Keras backend implementations\n",
    "def coef_det_k(y_true, y_pred): # order of variables defined in https://keras.io/backend/\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true-K.mean(y_true)))\n",
    "    return 1-SS_res/(SS_tot+K.epsilon())\n",
    "\n",
    "def corr_coef_k(y_true, y_pred):\n",
    "    xm, ym = y_true-K.mean(y_true), y_pred-K.mean(y_pred)\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    return K.maximum(K.minimum(r_num/(r_den+K.epsilon()), 1.0), -1.0)\n",
    "\n",
    "# numpy implementations\n",
    "def mse_np(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true-y_pred))\n",
    "\n",
    "def coef_det_np(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square(y_true-y_pred))\n",
    "    SS_tot = np.sum(np.square(y_true-np.mean(y_true)))\n",
    "    return 1-SS_res/(SS_tot+1e-7)\n",
    "\n",
    "def corr_coef_np(y_true, y_pred):\n",
    "    return np.corrcoef(y_pred[:,0],y_true[:,0])[0,1]\n",
    "\n",
    "# evaluations on test data \n",
    "def eval_on_test(X_test, Y_test, model, fname='', return_np=False):\n",
    "    loss = model.evaluate(X_test, Y_test, X_test[0].shape[0])\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    x = Y_pred[:,0]\n",
    "    y = Y_test[:,0]\n",
    "    slope, intercept, rvalue, pvalue, stderr = stats.linregress(x,y)\n",
    "    plt.plot(x, y, 'o', label='original data')\n",
    "    plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
    "    plt.legend()\n",
    "    plt.title('R2 = {}'.format(loss[1]))\n",
    "    plt.xlabel('Y_pred')\n",
    "    plt.ylabel('Y_true')\n",
    "    if len(fname)>0:\n",
    "        plt.savefig(fname+'.pdf', bbox_inches='tight')\n",
    "    \n",
    "    loss.append(rvalue)\n",
    "    \n",
    "    if return_np:\n",
    "        loss.append(mse_np(Y_test, Y_pred))\n",
    "        loss.append(coef_det_np(Y_test, Y_pred))\n",
    "        loss.append(corr_coef_np(Y_test, Y_pred))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#setting default path if interactive mode (run this cell only ONCE if in interactive mode)\n",
    "if is_interactive():\n",
    "    os.chdir(\"../\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import sys\n",
    "from os.path import basename\n",
    "\n",
    "#change only THESE\n",
    "model_path = \"./models/TEST_model_regression_1.py\"\n",
    "data_path = \"./data/THIS_data.npz\"\n",
    "\n",
    "\n",
    "weights_dir = \"./weights\"\n",
    "results_dir = \"./results\"\n",
    "model_name = os.path.splitext(basename(model_path))[0]\n",
    "weight_path = os.path.join(weights_dir, model_name)\n",
    "csv_logger_path = os.path.join(results_dir, model_name + \"_val_results.csv\")\n",
    "test_results_path = os.path.join(results_dir, model_name + \"_test_results.csv\")\n",
    "\n",
    "for filename in [weight_path, csv_logger_path, test_results_path]:\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "args = (sys.argv)\n",
    "config_path = \"\"\n",
    "if not is_interactive():\n",
    "    model_path = args[1]\n",
    "    config_path = args[2]\n",
    "    weight_path = args[3]\n",
    "    csv_logger_path = args[4]\n",
    "    test_results_path = args[4] + \"_testset\"\n",
    "    data_path = args[5]\n",
    "    \n",
    "#suffix = \"{epoch:03d}-{val_loss:.3f}.hdf5\"\n",
    "suffix = \"best.hdf5\"\n",
    "weight_model_path = \"{}.{}\".format(weight_path, suffix)\n",
    "suffix2 = \"last.hdf5\"\n",
    "weight_model_path_last = \"{}.{}\".format(weight_path, suffix2)\n",
    "\n",
    "\n",
    "#hyperparameters    \n",
    "DROPOUT = 0   # dropout\n",
    "ALPHA = 0.01 # learnrate\n",
    "EPOCHS = 100 # epochs\n",
    "MBATCH = 1 # batch size\n",
    "SHUFFLE = True\n",
    "LRS = True # learning rate scheduler activation\n",
    "\n",
    "#loading from config \n",
    "if not is_interactive():\n",
    "    config_file = args[2]\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    ALPHA = config.getfloat('main', 'alpha')\n",
    "    DROPOUT = config.getfloat('main', 'dropout')\n",
    "    EPOCHS = config.getint('main', 'epochs')\n",
    "    MBATCH = config.getint('main', 'mbatch')\n",
    "    LRS = config.getboolean('main', 'LRS')\n",
    "    SHUFFLE = config.getboolean('main', 'SHUFFLE')\n",
    "\n",
    "#loading model difinitions    \n",
    "model_file = open(model_path, 'r').read()\n",
    "exec(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "X_train, X_test, Y_train, Y_test = load_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zrimec/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1, use_bias=True, units=1)`\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train[0].shape[1:3]\n",
    "model = POC_model(input_shape, DROPOUT)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=ALPHA, beta_1=0.9, beta_2=0.999)\n",
    "#opt = SGD(lr=ALPHA)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=[coef_det_k, corr_coef_k])\n",
    "# lr_tmp = float(K.get_value(model.optimizer.lr))\n",
    "# print(lr_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callbacks\n",
    "\n",
    "# parameters\n",
    "MIN_DELTA = 0.1\n",
    "PATIENCE = 20\n",
    "LRS_DROP = 0.5\n",
    "LRS_EPOCH_DROP = 10\n",
    "LRS_TRESHOLD = 3\n",
    "\n",
    "# checkpoint\n",
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "# https://keras.io/callbacks/ - for now save every epoch\n",
    "\n",
    "check_best = ModelCheckpoint(weight_model_path, monitor='val_loss', verbose=0, \n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "check_last = ModelCheckpoint(weight_model_path_last, monitor='val_loss', verbose=0, \n",
    "                             save_best_only=False, save_weights_only=True, mode='auto')\n",
    "\n",
    "# tensorboard\n",
    "# http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "tensorboard = TrainValTensorBoard(write_graph=False, log_dir='./tensorboard_logs/' + \\\n",
    "                                  basename(data_path) + '_' + basename(csv_logger_path))\n",
    "\n",
    "csv = CSVLogger(csv_logger_path, separator = \",\", append = True)\n",
    "\n",
    "# early stopping\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=MIN_DELTA, patience=PATIENCE)\n",
    "\n",
    "# learning rate scheduler\n",
    "# add after certain amount of full training => treshold\n",
    "# A typical way is to to drop the learning rate by half every 10 epochs.\n",
    "# implements gradual decrease to drop by LRS_DROP every LRS_EPOCH_DROP epochs\n",
    "def schedule(epoch, lr):\n",
    "    treshold = LRS_TRESHOLD\n",
    "    drop = LRS_DROP\n",
    "    epoch_drop = LRS_EPOCH_DROP\n",
    "    if epoch > treshold:\n",
    "        lr *= pow(1-drop,1/epoch_drop)\n",
    "    return float(lr)\n",
    "\n",
    "if LRS:\n",
    "    ALPHA = LearningRateScheduler(schedule)\n",
    "    callbacks_list = [check_best, check_last, tensorboard, csv, earlystop, ALPHA]\n",
    "else:\n",
    "    callbacks_list = [check_best, check_last, tensorboard, csv, earlystop]\n",
    "\n",
    "# in terminal run: tensorboard --logdir=logs/\n",
    "# val_loss error is in callbacks, probably modelcheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if model exist then load best\n",
    "MODEL_LOAD = False\n",
    "\n",
    "if MODEL_LOAD:\n",
    "    # import glob\n",
    "    import re\n",
    "\n",
    "    # def find_best_model(all_models):\n",
    "    #         epochs = []\n",
    "    #         losses = []\n",
    "    #         for i, file in enumerate(all_models):\n",
    "    #             groups = re.findall(weight_path + '.(.*)-(.*).hdf5', file)\n",
    "    #             if groups:\n",
    "    #                 epochs.append(int(groups[0][0]))\n",
    "    #                 losses.append(float(groups[0][1]))\n",
    "    #         return (all_models[np.argmin(losses)] )\n",
    "\n",
    "    all_models = [os.path.join(os.path.dirname(weight_path), f) \\\n",
    "                  for f in os.listdir(os.path.dirname(weight_path)) \\\n",
    "                  if re.match(os.path.basename(weight_path) + '\\.'+suffix2, f)]\n",
    "\n",
    "    if all_models:\n",
    "        #best_model = find_best_model(all_models)\n",
    "        best_model = all_models[0]\n",
    "        print(\"Loading weights from {}\".format(best_model))\n",
    "        model.load_weights(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 1.9780 - coef_det_k: -19780094.2986 - corr_coef_k: 0.0000e+00 - val_loss: 0.6338 - val_coef_det_k: -6338192.1875 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.009999999776482582\n",
      "Epoch 2/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.2448 - coef_det_k: -2448490.5630 - corr_coef_k: 0.0000e+00 - val_loss: 0.0479 - val_coef_det_k: -479098.5039 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.009999999776482582\n",
      "Epoch 3/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0153 - coef_det_k: -152774.2022 - corr_coef_k: 0.0000e+00 - val_loss: 0.0030 - val_coef_det_k: -29664.0362 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.009999999776482582\n",
      "Epoch 4/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0030 - coef_det_k: -29787.3839 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18357.5897 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.009999999776482582\n",
      "Epoch 5/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27962.7577 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18608.2302 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.009330329485237598\n",
      "Epoch 6/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0027 - coef_det_k: -27479.5718 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18290.9296 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.008705505169928074\n",
      "Epoch 7/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27694.9229 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18345.5123 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.008122523315250874\n",
      "Epoch 8/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27705.4693 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18662.5846 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.007578582037240267\n",
      "Epoch 9/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27539.4896 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18526.4243 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.007071067113429308\n",
      "Epoch 10/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27500.7524 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18478.2690 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.006597538944333792\n",
      "Epoch 11/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0027 - coef_det_k: -27408.8108 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18300.9166 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.006155721377581358\n",
      "Epoch 12/100\n",
      "144/144 [==============================] - 0s 2ms/step - loss: 0.0028 - coef_det_k: -27636.1272 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18781.7631 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.005743490997701883\n",
      "Epoch 13/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27588.1447 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18726.6935 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.005358866415917873\n",
      "Epoch 14/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27725.8799 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18714.8375 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.004999998956918716\n",
      "Epoch 15/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27752.9523 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -18048.0295 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.004665163811296225\n",
      "Epoch 16/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0027 - coef_det_k: -27491.2858 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18528.1585 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.004352751653641462\n",
      "Epoch 17/100\n",
      "144/144 [==============================] - 0s 2ms/step - loss: 0.0027 - coef_det_k: -27471.8677 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -17915.2183 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.004061260726302862\n",
      "Epoch 18/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27791.4700 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -17782.3664 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.0037892903201282024\n",
      "Epoch 19/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27612.7848 - corr_coef_k: 0.0000e+00 - val_loss: 0.0019 - val_coef_det_k: -18948.7330 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.003535532858222723\n",
      "Epoch 20/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0027 - coef_det_k: -27476.7182 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -17924.4282 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.003298768773674965\n",
      "Epoch 21/100\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.0028 - coef_det_k: -27685.6483 - corr_coef_k: 0.0000e+00 - val_loss: 0.0020 - val_coef_det_k: -19906.5197 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.003077859990298748\n",
      "Epoch 22/100\n",
      "144/144 [==============================] - 0s 2ms/step - loss: 0.0028 - coef_det_k: -27899.6770 - corr_coef_k: 0.0000e+00 - val_loss: 0.0018 - val_coef_det_k: -17975.1656 - val_corr_coef_k: 0.0000e+00\n",
      "Learning rate: 0.0028717448003590107\n",
      "4.83073616027832\n"
     ]
    }
   ],
   "source": [
    "# keras model checkpoint KeyError: 'val_loss'\n",
    "# fix: https://github.com/keras-team/keras/issues/6104\n",
    "# must add validation_split=xx\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=MBATCH, \n",
    "          epochs=EPOCHS, \n",
    "          validation_split=0.1, \n",
    "          shuffle=SHUFFLE, \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "# tests data on test set \n",
    "test_loss = eval_on_test(X_test, Y_test, model, fname=test_results_path, return_np=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "d = {'MSE' : [test_loss[0]],\n",
    "     'coef_determination' : [test_loss[1]], \n",
    "     'corr_coef' : [test_loss[2]],\n",
    "     'corr_coef_plot' : [test_loss[3]]}\n",
    "\n",
    "test_df = pd.DataFrame(data=d)\n",
    "test_df.to_csv(test_results_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0031794372480362654,\n",
       " 0.9585642218589783,\n",
       " 0.9802033305168152,\n",
       " 0.9802032535679895,\n",
       " 0.0031794368774013273,\n",
       " 0.9585642077487304,\n",
       " 0.9802032535679895]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
